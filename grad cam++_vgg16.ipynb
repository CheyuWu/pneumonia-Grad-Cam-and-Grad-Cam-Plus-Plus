{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img #import image need this\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense,Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import tensorflow as  tf\n",
    "import keras.backend as K\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_curve,roc_auc_score,auc\n",
    "from keras.layers.advanced_activations import LeakyReLU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(results,img_name): \n",
    "    # list all data in history\n",
    "    print(results.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(1)\n",
    "    plt.plot(results.history['acc'])\n",
    "    plt.plot(results.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('Accuracy_'+img_name,dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(2)\n",
    "    plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.argmin(results.history[\"val_loss\"]), \n",
    "             np.min(results.history[\"val_loss\"]), \n",
    "             marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"log_loss\")\n",
    "    plt.legend();\n",
    "    plt.savefig('Loss_'+img_name ,dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flow_from_directory目錄需要分3類\n",
    "train_image = './pneu_dataset/train'\n",
    "valid_image = './pneu_dataset/validation'\n",
    "test_image = './pneu_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the default value\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224 , 224\n",
    "batch_size = 32\n",
    "epochs = 700\n",
    "seed = 42\n",
    "class_mode = 'categorical'\n",
    "date = '2019-06-22_VGG16_final'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ImageDataGenerator\n",
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range = 180)  #圖片隨機轉動\n",
    "#                              width_shift_range = 0.2, #圖片水平偏移\n",
    "#                              height_shift_range = 0.2, #圖片垂直偏移\n",
    "#                              zoom_range = 0.3)\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "        train_image,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        shuffle=True,\n",
    "        class_mode = class_mode,\n",
    "        seed = seed)\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        valid_image,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode = class_mode,\n",
    "        shuffle = False ,\n",
    "        seed = seed)\n",
    "\n",
    "# test_generator = datagen.flow_from_directory(\n",
    "#         test_image,\n",
    "#         target_size = (img_width, img_height),\n",
    "#         batch_size = 1,\n",
    "#         class_mode = None,\n",
    "#         shuffle = False,\n",
    "#         seed = seed\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_generator[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Model way transfer learning\n",
    "# initial_model =VGG16(include_top=False,weights='imagenet')\n",
    "# input = Input(shape=(224,224,3),name = 'image_input')\n",
    "# output_vgg16_conv = initial_model(input)\n",
    "\n",
    "# x = Flatten()(output_vgg16_conv)\n",
    "# x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "# x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "# x = Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# model = Model(input= input ,output = x)\n",
    "\n",
    "# Transfer learning\n",
    "# model = Sequential()\n",
    "# for layer in VGG16().layers:\n",
    "#     model.add(layer)\n",
    "# model.layers.pop()\n",
    "# model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(4096, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(4096, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG_16('./vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定checkpoint & callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "def callbacks(name):\n",
    "    save_dir = \"checkpoint\"\n",
    "    #     tl.files.exists_or_mkdir(save_dir)\n",
    "    path=save_dir+'/model_check_'+name+'.h5'\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=15, verbose=1),\n",
    "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.000001, verbose=1),\n",
    "        ModelCheckpoint(path, verbose=1, save_best_only=True, save_weights_only=False)\n",
    "#         ModelCheckpoint(path,monitor='val_acc', verbose=1,\n",
    "#                         save_best_only=True, save_weights_only=False)\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_samples  = train_generator.classes.shape[0]\n",
    "num_of_test_samples = validation_generator.classes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# model= load_model('./checkpoint/model_check_2019-06-22_VGG16_final_fit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= num_of_train_samples//batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= num_of_test_samples//batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks=callbacks(date+'_fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history,'2019-6-22_vgg16_with_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CheckPoint\n",
    "# from keras.models import load_model\n",
    "# model= load_model('./checkpoint/model_check_2019-03-14_1_fit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.evaluate_generator(generator=validation_generator,\n",
    "#                          steps=validation_generator.classes.shape[0]//32 +1,\n",
    "                         steps=validation_generator.classes.shape[0],\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict the output\n",
    "Y_pred = model.predict_generator(validation_generator,\n",
    "                                 steps=num_of_test_samples//32 +1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use auc to show the performance\n",
    "# auc = roc_auc_score(validation_generator.classes,Y_pred)\n",
    "# print(\"AUC: {}\".format(auc))\n",
    "# fpr, tpr, thresholds = roc_curve(validation_generator.classes, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot ROC curve\n",
    "# plt.figure(3)\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.plot(fpr,tpr,label='AUC = %0.2f' % auc)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# # plt.show()\n",
    "# plt.savefig(\"roc_curve.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CheckPoint\n",
    "# from keras.models import load_model \n",
    "# base_model= load_model('./checkpoint/model_check_2019-06-16_VGG16_fit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "plt.rcParams['figure.figsize'] = 8,8\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import os\n",
    "import gradcamutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions_cust(preds, class_custom, top=3):\n",
    "    results = []\n",
    "\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [tuple(class_custom[i][0])+(class_custom[i][1],) + (pred[i]*100,)\\\n",
    "                  for i in top_indices] \n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "class_name=[('0','Lung Opacity'),('1','Normal'),('2','Not Normal')]\n",
    "dataset_path = 'validation'\n",
    "label = [] # label Lung Opacity, Normal, Not Normal\n",
    "# path of img \n",
    "valid_lung_op_path='./pneu_dataset/'+dataset_path+'/Lung Opacity/'\n",
    "valid_normal_path='./pneu_dataset/'+dataset_path+'/Normal/'\n",
    "valid_not_normal_path='./pneu_dataset/'+dataset_path+'/Not Normal/'\n",
    "paths = [valid_lung_op_path,valid_normal_path,valid_not_normal_path]\n",
    "# list of each img\n",
    "valid_path = []\n",
    "# dir \n",
    "for path in paths:\n",
    "    for file in  os.listdir(path):\n",
    "        if path == './pneu_dataset/'+dataset_path+'/Lung Opacity/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Lung Opacity')\n",
    "        elif path == './pneu_dataset/'+dataset_path+'/Normal/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Normal')\n",
    "        elif path == './pneu_dataset/'+dataset_path+'/Not Normal/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Not Normal')\n",
    "        else :\n",
    "            label.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.get_input_at(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use grad cam & ++ \n",
    "for path in valid_path:\n",
    "    count+=1\n",
    "    if count <= 5000 : # change other data type\n",
    "        pass\n",
    "    else :\n",
    "        orig_img = np.array(load_img(path,target_size=(224,224)),dtype=np.uint8)\n",
    "        img = np.array(load_img(path,target_size=(224,224)),dtype=np.float64)\n",
    "        img = np.expand_dims(img,axis=0)\n",
    "\n",
    "        img = preprocess_input(img)\n",
    "        predictions = base_model.predict(img)\n",
    "        top_n = 3\n",
    "        top = decode_predictions_cust(predictions,class_name ,top=top_n)[0]\n",
    "    #     top = decode_predictions(predictions, top=top_n)[0]\n",
    "        cls = np.argsort(predictions[0])[-top_n:][::-1]\n",
    "\n",
    "        gradcam=gradcamutils.grad_cam(base_model,img,layer_name='conv2d_13')\n",
    "        gradcamplus=gradcamutils.grad_cam_plus(base_model,img,layer_name='conv2d_13')\n",
    "        print(path)\n",
    "        print(\"class activation map for:\",top[0])\n",
    "        fig, ax = plt.subplots(nrows=1,ncols=3)\n",
    "        plt.subplot(131)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.title(\"input image \\n\"+'('+str(label[count])+')')\n",
    "        plt.subplot(132)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.imshow(gradcam,alpha=0.4,cmap=\"jet\")\n",
    "        plt.title(\"Grad-CAM \\n\"+str(top[0][1])+'\\n'+str(top[0][2])[:6])\n",
    "\n",
    "        plt.subplot(133)\n",
    "        plt.imshow(orig_img)\n",
    "        plt.imshow(gradcamplus,alpha=0.4,cmap=\"jet\")\n",
    "        plt.title(\"Grad-CAM++ \\n\"+str(top[0][1])+'\\n'+str(top[0][2])[:6])\n",
    "        plt.savefig('./grad_cam_image/VGG16/VGG16_gram&++_'+str(count)+'.png',dpi=300)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad Cam another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "def load_image(path,target_size=(256, 256)):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_pool'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def grad_cam(model, x, category_index, layer_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       model: model\n",
    "       x: image input\n",
    "       category_index: category index\n",
    "       layer_name: last convolution layer name\n",
    "    \"\"\"\n",
    "    # get category loss\n",
    "    class_output = model.output[:, category_index]\n",
    "\n",
    "    # layer output\n",
    "    convolution_output = model.get_layer(layer_name).output\n",
    "    # get gradients\n",
    "    grads = K.gradients(class_output, convolution_output)[0]\n",
    "    # get convolution output and gradients for input\n",
    "    gradient_function = K.function([model.input], [convolution_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([x])\n",
    "    output, grads_val = output[0], grads_val[0]\n",
    "\n",
    "    # avg\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # create heat map\n",
    "    cam = cv2.resize(cam, (x.shape[1], x.shape[2]), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    # Return to BGR [0..255] from the preprocessed image\n",
    "    image_rgb = x[0, :]\n",
    "    image_rgb -= np.min(image_rgb)\n",
    "    image_rgb = np.minimum(image_rgb, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image_rgb)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_cam_folder = './grad_cam_image/'\n",
    "# class_name=[('0','Lung Opacity'),('1','Normal'),('2','Not Normal')]\n",
    "\n",
    "# arr_images = []\n",
    "\n",
    "# for i, path in enumerate(valid_path):\n",
    "#     img = load_image(path,(256,256))\n",
    "\n",
    "#     predictions = base_model.predict(img)\n",
    "#     top_1 = decode_predictions_cust(predictions,class_name)[0][0]\n",
    "#     print('Predicted class:')\n",
    "#     print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "#     predicted_class = np.argmax(predictions)\n",
    "#     cam_image, heat_map = grad_cam(base_model, img, predicted_class, \"conv2d_3\")\n",
    "    \n",
    "#     img_file = image.load_img(path)\n",
    "#     img_file = image.img_to_array(img_file)\n",
    "\n",
    "#     # guided grad_cam img\n",
    "#     register_gradient()\n",
    "#     guided_model = modify_backprop(base_model, 'GuidedBackProp')\n",
    "#     saliency_fn = compile_saliency_function(guided_model)\n",
    "\n",
    "#     saliency = saliency_fn([img, 0])\n",
    "#     grad_cam_img = saliency[0] * heat_map[..., np.newaxis]\n",
    "#  # save img\n",
    "\n",
    "#     cam_image = cv2.resize(cam_image, (img_file.shape[1], img_file.shape[0]), cv2.INTER_LINEAR)\n",
    "#     cv2.putText(cam_image,str(top_1[1]), (20, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "#     cv2.putText(cam_image,str(top_1[2]), (20, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "\n",
    "#     grad_cam_img = deprocess_image(grad_cam_img)\n",
    "#     grad_cam_img = cv2.resize(grad_cam_img, (img_file.shape[1], img_file.shape[0]), cv2.INTER_LINEAR)\n",
    "#     cv2.putText(grad_cam_img,str(top_1[1]), (20, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "#     cv2.putText(grad_cam_img,str(top_1[2]), (20, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "\n",
    "#     cam_image = cam_image.astype('float32')\n",
    "#     grad_cam_img = grad_cam_img.astype('float32')\n",
    "#     im_h = cv2.hconcat([img_file, cam_image, grad_cam_img])\n",
    "#     cv2.imwrite(pic_cam_folder +top_1[1] +'_'+str(i)+'.jpg', im_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
