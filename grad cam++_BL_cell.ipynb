{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img #import image need this\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense,Input\n",
    "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import VGG19\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications import MobileNetV2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "import tensorflow as  tf\n",
    "import keras.backend as K\n",
    "import os\n",
    "import random\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report, confusion_matrix,roc_curve,roc_auc_score,auc\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def history_plot(results): \n",
    "    # list all data in history\n",
    "    print(results.history.keys())\n",
    "    # summarize history for accuracy\n",
    "    plt.figure(1)\n",
    "    plt.plot(results.history['acc'])\n",
    "    plt.plot(results.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.savefig('Accuracy',dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.figure(2)\n",
    "    plt.plot(results.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(results.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(np.argmin(results.history[\"val_loss\"]), \n",
    "             np.min(results.history[\"val_loss\"]), \n",
    "             marker=\"x\", color=\"r\", label=\"best model\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"log_loss\")\n",
    "    plt.legend();\n",
    "    plt.savefig('Loss',dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flow_from_directory目錄需要分3類\n",
    "train_image = './pneu_dataset/train'\n",
    "valid_image = './pneu_dataset/validation'\n",
    "test_image = './pneu_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setting the default value\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224 , 224\n",
    "batch_size = 32\n",
    "epochs = 300\n",
    "seed = 42\n",
    "class_mode = 'categorical'\n",
    "date = '2019-06-16_BL_cell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19689 images belonging to 3 classes.\n",
      "Found 6471 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#ImageDataGenerator\n",
    "# used to rescale the pixel values from [0, 255] to [0, 1] interval\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "#                              rotation_range = 90,  #圖片隨機轉動\n",
    "#                              width_shift_range = 0.2, #圖片水平偏移\n",
    "#                              height_shift_range = 0.2, #圖片垂直偏移\n",
    "#                              zoom_range = 0.3)\n",
    "\n",
    "\n",
    "# automagically retrieve images and their classes for train and validation sets\n",
    "train_generator = datagen.flow_from_directory(\n",
    "       train_image,\n",
    "        target_size = (img_width, img_height),\n",
    "        batch_size = batch_size,\n",
    "        class_mode = class_mode,\n",
    "        seed = seed)\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "        valid_image,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode = class_mode,\n",
    "        shuffle = False ,\n",
    "        seed = seed)\n",
    "\n",
    "# test_generator = datagen.flow_from_directory(\n",
    "#         test_image,\n",
    "#         target_size = (img_width, img_height),\n",
    "#         batch_size = 1,\n",
    "#         class_mode = None,\n",
    "#         shuffle = False,\n",
    "#         seed = seed\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cheyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/cheyu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cheyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(224, 224,...)`\n",
      "  \n",
      "/home/cheyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "/home/cheyu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n"
     ]
    }
   ],
   "source": [
    "# ## Model way transfer learning\n",
    "# initial_model =VGG16(include_top=False,weights='imagenet')\n",
    "# input = Input(shape=(128,128,3),name = 'image_input')\n",
    "# output_vgg16_conv = initial_model(input)\n",
    "\n",
    "# x = Flatten()(output_vgg16_conv)\n",
    "# x = Dense(4096, activation='relu', name='fc1')(x)\n",
    "# x = Dense(4096, activation='relu', name='fc2')(x)\n",
    "# x = Dense(3, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# model = Model(input= input ,output = x)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Transfer learning\n",
    "# model = Sequential()\n",
    "# for layer in VGG16().layers:\n",
    "#     model.add(layer)\n",
    "# model.layers.pop()\n",
    "# model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## John model\n",
    "# model = Sequential()\n",
    "# model.add(Convolution2D(64, 3, 3, input_shape=(img_width, img_height,3)))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Convolution2D(128, 3, 3))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Convolution2D(256, 3, 3))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(128))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(3))\n",
    "# model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 109, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 52, 52, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2768960   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 2,797,795\n",
      "Trainable params: 2,797,795\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定checkpoint & callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "def callbacks(name):\n",
    "    save_dir = \"checkpoint\"\n",
    "    #     tl.files.exists_or_mkdir(save_dir)\n",
    "    path=save_dir+'/model_check_'+name+'.h5'\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=35, verbose=1),\n",
    "        ReduceLROnPlateau(factor=0.1, patience=10, min_lr=0.000001, verbose=1),\n",
    "        ModelCheckpoint(path, verbose=1, save_best_only=True, save_weights_only=False)\n",
    "#         ModelCheckpoint(path,monitor='val_acc', verbose=1,\n",
    "#                         save_best_only=True, save_weights_only=False)\n",
    "    ]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_samples  = train_generator.classes.shape[0]\n",
    "num_of_test_samples = validation_generator.classes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch= batch_size,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps= batch_size,\n",
    "        epochs = epochs,\n",
    "        callbacks=callbacks(date+'_fit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CheckPoint\n",
    "# from keras.models import load_model\n",
    "# model= load_model('./checkpoint/model_check_2019-03-14_1_fit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "model.evaluate_generator(generator=validation_generator,\n",
    "#                          steps=validation_generator.classes.shape[0]//32 +1,\n",
    "                         steps=validation_generator.classes.shape[0],\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict the output\n",
    "Y_pred = model.predict_generator(validation_generator,\n",
    "                                 steps=num_of_test_samples//32 +1,\n",
    "                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use auc to show the performance\n",
    "# auc = roc_auc_score(validation_generator.classes,Y_pred)\n",
    "# print(\"AUC: {}\".format(auc))\n",
    "# fpr, tpr, thresholds = roc_curve(validation_generator.classes, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot ROC curve\n",
    "# plt.figure(3)\n",
    "# plt.plot([0, 1], [0, 1], 'k--')\n",
    "# plt.plot(fpr,tpr,label='AUC = %0.2f' % auc)\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('ROC Curve')\n",
    "# # plt.show()\n",
    "# plt.savefig(\"roc_curve.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cheyu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "# Load CheckPoint\n",
    "from keras.models import load_model \n",
    "base_model= load_model('./checkpoint/model_check_2019-06-16_BL_cell_fit.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'png', 'retina'}\n",
    "plt.rcParams['figure.figsize'] = 8,8\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from tensorflow.python.framework import ops\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "import cv2\n",
    "import os\n",
    "import gradcamutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most import value\n",
    "def decode_predictions_cust(preds, class_custom, top=3):\n",
    "    results = []\n",
    "\n",
    "    for pred in preds:\n",
    "        top_indices = pred.argsort()[-top:][::-1]\n",
    "        result = [tuple(class_custom[i][0])+(class_custom[i][1],) + (pred[i]*100,)\\\n",
    "                  for i in top_indices] \n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "class_name=[('0','Lung Opacity'),('1','Normal'),('2','Not Normal')]\n",
    "label = [] # label Lung Opacity, Normal, Not Normal\n",
    "# path of img \n",
    "valid_lung_op_path='./pneu_dataset/validation/Lung Opacity/'\n",
    "valid_normal_path='./pneu_dataset/validation/Normal/'\n",
    "valid_not_normal_path='./pneu_dataset/validation/Not Normal/'\n",
    "paths = [valid_lung_op_path,valid_normal_path,valid_not_normal_path]\n",
    "# list of each img\n",
    "valid_path = []\n",
    "# dir \n",
    "for path in paths:\n",
    "    for file in  os.listdir(path):\n",
    "        if path == './pneu_dataset/validation/Lung Opacity/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Lung Opacity')\n",
    "        elif path == './pneu_dataset/validation/Normal/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Normal')\n",
    "        elif path == './pneu_dataset/validation/Not Normal/':\n",
    "            valid_path.append(path+file)\n",
    "            label.append('Not Normal')\n",
    "        else :\n",
    "            label.append('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have shape (128, 128, 3) but got array with shape (224, 224, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bb1c9e35c95b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtop_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions_cust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_name\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have shape (128, 128, 3) but got array with shape (224, 224, 3)"
     ]
    }
   ],
   "source": [
    "# use grad cam & ++ \n",
    "for path in valid_path:\n",
    "    count+=1\n",
    "    orig_img = np.array(load_img(path,target_size=(224,224)),dtype=np.uint8)\n",
    "    img = np.array(load_img(path,target_size=(224,224)),dtype=np.float64)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    predictions = base_model.predict(img)\n",
    "    top_n = 3\n",
    "    top = decode_predictions_cust(predictions,class_name ,top=top_n)[0]\n",
    "#     top = decode_predictions(predictions, top=top_n)[0]\n",
    "    cls = np.argsort(predictions[0])[-top_n:][::-1]\n",
    "    \n",
    "    gradcam=gradcamutils.grad_cam(base_model,img,layer_name='conv2d_3')\n",
    "    gradcamplus=gradcamutils.grad_cam_plus(base_model,img,layer_name='conv2d_3')\n",
    "    print(path)\n",
    "    print(\"class activation map for:\",top[0])\n",
    "    fig, ax = plt.subplots(nrows=1,ncols=3)\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(orig_img)\n",
    "    plt.title(\"input image \\n\"+'('+str(label[count])+')')\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(orig_img)\n",
    "    plt.imshow(gradcam,alpha=0.4,cmap=\"jet\")\n",
    "    plt.title(\"Grad-CAM \\n\"+str(top[0][1])+'\\n'+str(top[0][2])[:6])\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(orig_img)\n",
    "    plt.imshow(gradcamplus,alpha=0.4,cmap=\"jet\")\n",
    "    plt.title(\"Grad-CAM++ \\n\"+str(top[0][1])+'\\n'+str(top[0][2])[:6])\n",
    "#     plt.savefig('./grad_cam_image/bloodcell/BL_gram&++_'+str(count)+'.png',dpi=300)\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcamplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grad cam only another method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "\n",
    "def load_image(path,target_size=(256, 256)):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='block5_pool'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = VGG16(weights='imagenet')\n",
    "    return new_model\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_dim_ordering() == 'th':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def grad_cam(model, x, category_index, layer_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "       model: model\n",
    "       x: image input\n",
    "       category_index: category index\n",
    "       layer_name: last convolution layer name\n",
    "    \"\"\"\n",
    "    # get category loss\n",
    "    class_output = model.output[:, category_index]\n",
    "\n",
    "    # layer output\n",
    "    convolution_output = model.get_layer(layer_name).output\n",
    "    # get gradients\n",
    "    grads = K.gradients(class_output, convolution_output)[0]\n",
    "    # get convolution output and gradients for input\n",
    "    gradient_function = K.function([model.input], [convolution_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([x])\n",
    "    output, grads_val = output[0], grads_val[0]\n",
    "\n",
    "    # avg\n",
    "    weights = np.mean(grads_val, axis=(0, 1))\n",
    "    cam = np.dot(output, weights)\n",
    "\n",
    "    # create heat map\n",
    "    cam = cv2.resize(cam, (x.shape[1], x.shape[2]), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    # Return to BGR [0..255] from the preprocessed image\n",
    "    image_rgb = x[0, :]\n",
    "    image_rgb -= np.min(image_rgb)\n",
    "    image_rgb = np.minimum(image_rgb, 255)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam) + np.float32(image_rgb)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pic_cam_folder = './grad_cam_image/bloodcell/'\n",
    "\n",
    "\n",
    "# arr_images = []\n",
    "\n",
    "# for i, path in enumerate(valid_path):\n",
    "#     img = load_image(path,(128,128))\n",
    "\n",
    "#     predictions = base_model.predict(img)\n",
    "#     top_1 = decode_predictions_cust(predictions,class_name)[0][0]\n",
    "#     print('Predicted class:')\n",
    "#     print('%s (%s) with probability %.2f' % (top_1[1], top_1[0], top_1[2]))\n",
    "\n",
    "#     predicted_class = np.argmax(predictions)\n",
    "#     cam_image, heat_map = grad_cam(base_model, img, predicted_class, \"conv2d_3\")\n",
    "    \n",
    "#     img_file = image.load_img(path)\n",
    "#     img_file = image.img_to_array(img_file)\n",
    "\n",
    "#     # guided grad_cam img\n",
    "#     register_gradient()\n",
    "#     guided_model = modify_backprop(base_model, 'GuidedBackProp')\n",
    "#     saliency_fn = compile_saliency_function(guided_model)\n",
    "\n",
    "#     saliency = saliency_fn([img, 0])\n",
    "#     grad_cam_img = saliency[0] * heat_map[..., np.newaxis]\n",
    "#  # save img\n",
    "\n",
    "#     cam_image = cv2.resize(cam_image, (img_file.shape[1], img_file.shape[0]), cv2.INTER_LINEAR)\n",
    "#     cv2.putText(cam_image,str(top_1[1]), (20, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "#     cv2.putText(cam_image,str(top_1[2]), (20, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "\n",
    "#     grad_cam_img = deprocess_image(grad_cam_img)\n",
    "#     grad_cam_img = cv2.resize(grad_cam_img, (img_file.shape[1], img_file.shape[0]), cv2.INTER_LINEAR)\n",
    "#     cv2.putText(grad_cam_img,str(top_1[1]), (20, 20), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "#     cv2.putText(grad_cam_img,str(top_1[2]), (20, 50), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1,(0, 0, 255))\n",
    "\n",
    "#     cam_image = cam_image.astype('float32')\n",
    "#     grad_cam_img = grad_cam_img.astype('float32')\n",
    "#     im_h = cv2.hconcat([img_file, cam_image, grad_cam_img])\n",
    "#     cv2.imwrite(pic_cam_folder +top_1[1] +'_'+str(i)+'.jpg', im_h)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
